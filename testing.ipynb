{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1c0209",
   "metadata": {},
   "source": [
    "# Coal Inventory Scraper for Chinese Ports\n",
    "\n",
    "This notebook contains a Python script that scrapes coal inventory data from the sxcoal.com website. It specifically targets articles containing \"煤炭库存\" (coal inventory) to extract date, port name, and inventory figures. The extracted data is then reshaped into a table and saved as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4081493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing browser for scrape-and-export task ---\n",
      "Navigating to: https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\n",
      "Navigating to: https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\n",
      "\n",
      "Finding all relevant inventory articles...\n",
      ">>> Success! Found 16 unique articles to process.\n",
      "\n",
      "--- Processing article 1 of 16 ---\n",
      "\n",
      "Finding all relevant inventory articles...\n",
      ">>> Success! Found 16 unique articles to process.\n",
      "\n",
      "--- Processing article 1 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 2 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 2 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '黄骅港' with inventory 175.6\n",
      "\n",
      "--- Processing article 3 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '黄骅港' with inventory 175.6\n",
      "\n",
      "--- Processing article 3 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 4 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 4 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '黄骅港' with inventory 180.3\n",
      "\n",
      "--- Processing article 5 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '黄骅港' with inventory 180.3\n",
      "\n",
      "--- Processing article 5 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 6 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 6 of 16 ---\n",
      "  >>> SUCCESS: Found [7月4日] '秦皇岛港' with inventory 570\n",
      "\n",
      "--- Processing article 7 of 16 ---\n",
      "  >>> SUCCESS: Found [7月4日] '秦皇岛港' with inventory 570\n",
      "\n",
      "--- Processing article 7 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 8 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 8 of 16 ---\n",
      "  >>> SUCCESS: Found [7月7日] '秦皇岛港' with inventory 576\n",
      "\n",
      "--- Processing article 9 of 16 ---\n",
      "  >>> SUCCESS: Found [7月7日] '秦皇岛港' with inventory 576\n",
      "\n",
      "--- Processing article 9 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '秦皇岛港' with inventory 575\n",
      "\n",
      "--- Processing article 10 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '秦皇岛港' with inventory 575\n",
      "\n",
      "--- Processing article 10 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '黄骅港' with inventory 182.5\n",
      "\n",
      "--- Processing article 11 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '黄骅港' with inventory 182.5\n",
      "\n",
      "--- Processing article 11 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 12 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 12 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '秦皇岛港' with inventory 573\n",
      "\n",
      "--- Processing article 13 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '秦皇岛港' with inventory 573\n",
      "\n",
      "--- Processing article 13 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '黄骅港' with inventory 189.3\n",
      "\n",
      "--- Processing article 14 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '黄骅港' with inventory 189.3\n",
      "\n",
      "--- Processing article 14 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '京唐港' with inventory 701\n",
      "\n",
      "--- Processing article 15 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '京唐港' with inventory 701\n",
      "\n",
      "--- Processing article 15 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '曹妃甸港' with inventory 1254\n",
      "\n",
      "--- Processing article 16 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '曹妃甸港' with inventory 1254\n",
      "\n",
      "--- Processing article 16 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Data extraction complete. Reshaping table... ---\n",
      "\n",
      "--- Reshaping complete. Exporting data to 'coal_inventory_data.csv'... ---\n",
      ">>> SUCCESS: Data has been saved to coal_inventory_data.csv\n",
      "\n",
      "Script finished. Closing browser...\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Data extraction complete. Reshaping table... ---\n",
      "\n",
      "--- Reshaping complete. Exporting data to 'coal_inventory_data.csv'... ---\n",
      ">>> SUCCESS: Data has been saved to coal_inventory_data.csv\n",
      "\n",
      "Script finished. Closing browser...\n",
      "\n",
      "\n",
      "--- FINAL DATA PREVIEW ---\n",
      "\n",
      "\n",
      "--- FINAL DATA PREVIEW ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Port</th>\n",
       "      <th>Date</th>\n",
       "      <th>京唐港</th>\n",
       "      <th>曹妃甸港</th>\n",
       "      <th>秦皇岛港</th>\n",
       "      <th>黄骅港</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-09-25</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>189.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-08-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575.0</td>\n",
       "      <td>182.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-07-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>576.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-04-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580.0</td>\n",
       "      <td>180.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580.0</td>\n",
       "      <td>175.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Port      Date    京唐港    曹妃甸港   秦皇岛港    黄骅港\n",
       "0     07-09-25  701.0  1254.0  573.0  189.3\n",
       "1     07-08-25    NaN     NaN  575.0  182.5\n",
       "2     07-07-25    NaN     NaN  576.0    NaN\n",
       "3     07-04-25    NaN     NaN  570.0    NaN\n",
       "4     07-03-25    NaN     NaN  580.0  180.3\n",
       "5     07-02-25    NaN     NaN  580.0  175.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## need to install undetected_chromedriver and pandas if not already installed\n",
    "# This script scrapes coal inventory data from a specific website, reshapes it, and exports\n",
    "# the final table to a CSV file. It uses Selenium for web scraping and Pandas for data manipulation.\n",
    "# File: coal_inventory_scraper.py\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "except ImportError:\n",
    "    print(\"Required libraries not found. Installing them now...\")\n",
    "    %pip install undetected-chromedriver pandas\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_export_inventory_data():\n",
    "    \"\"\"\n",
    "    Final version: Scrapes all inventory articles, reshapes the data,\n",
    "    and exports the final table to a CSV file.\n",
    "    \"\"\"\n",
    "    search_url = \"https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\"\n",
    "    csv_filename = \"coal_inventory_data.csv\"\n",
    "    raw_data_list = []\n",
    "\n",
    "    print(\"--- Initializing browser for scrape-and-export task ---\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "        driver = uc.Chrome(options=options, use_subprocess=False)\n",
    "        wait = WebDriverWait(driver, 25) \n",
    "\n",
    "        # Step 1: Gather URLs\n",
    "        print(f\"Navigating to: {search_url}\")\n",
    "        driver.get(search_url)\n",
    "\n",
    "        print(\"\\nFinding all relevant inventory articles...\")\n",
    "        article_link_selector = (By.PARTIAL_LINK_TEXT, \"煤炭库存\")\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable(article_link_selector))\n",
    "            article_elements = driver.find_elements(*article_link_selector)\n",
    "            article_urls = sorted(list(set([el.get_attribute('href') for el in article_elements])))\n",
    "            print(f\">>> Success! Found {len(article_urls)} unique articles to process.\")\n",
    "        except TimeoutException:\n",
    "            print(\"\\n--- FAILURE ---\")\n",
    "            print(\"Could not find any article links containing '煤炭库存'.\")\n",
    "            return None \n",
    "\n",
    "        # Step 2: Loop through URLs and extract data\n",
    "        for i, url in enumerate(article_urls):\n",
    "            print(f\"\\n--- Processing article {i+1} of {len(article_urls)} ---\")\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(3) \n",
    "                page_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "                pattern = re.compile(r\"(\\d+月\\d+日)，([\\u4e00-\\u9fa5]+港)煤炭库存为(\\d+\\.?\\d*)\")\n",
    "                match = pattern.search(page_text)\n",
    "                \n",
    "                if match:\n",
    "                    date_str, port_name, inventory = match.groups()\n",
    "                    raw_data_list.append({\"Date\": date_str, \"Port\": port_name.strip(), \"Inventory\": float(inventory)})\n",
    "                    print(f\"  >>> SUCCESS: Found [{date_str}] '{port_name.strip()}' with inventory {inventory}\")\n",
    "                else:\n",
    "                    print(f\"  > INFO: No data matching the required pattern was found.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  > ERROR: An error occurred processing this article: {type(e).__name__}\")\n",
    "        \n",
    "        if not raw_data_list:\n",
    "            print(\"\\nScraping finished, but no data could be extracted.\")\n",
    "            return None\n",
    "            \n",
    "        # Step 3: Reshape the data\n",
    "        print(\"\\n--- Data extraction complete. Reshaping table... ---\")\n",
    "        long_df = pd.DataFrame(raw_data_list)\n",
    "        current_year = datetime.now().year\n",
    "        long_df['FullDate'] = long_df['Date'].apply(lambda x: datetime.strptime(f'{current_year}年{x}', '%Y年%m月%d日'))\n",
    "\n",
    "        final_table = long_df.pivot_table(index='FullDate', columns='Port', values='Inventory')\n",
    "        final_table.sort_index(ascending=False, inplace=True)\n",
    "        final_table.reset_index(inplace=True)\n",
    "        final_table.rename(columns={'FullDate': 'Date'}, inplace=True)\n",
    "        final_table['Date'] = final_table['Date'].dt.strftime('%m-%d-%y')\n",
    "        \n",
    "        # ***4: EXPORT TO CSV ***\n",
    "        print(f\"\\n--- Reshaping complete. Exporting data to '{csv_filename}'... ---\")\n",
    "        try:\n",
    "            # We use index=False to avoid writing the pandas row numbers (0, 1, 2...) into the file.\n",
    "            # encoding='utf-8-sig' helps Excel open the file correctly with Chinese characters.\n",
    "            final_table.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\">>> SUCCESS: Data has been saved to {csv_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"--- FAILED TO SAVE FILE: An error occurred: {e} ---\")\n",
    "\n",
    "        return final_table\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nA critical error occurred: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"\\nScript finished. Closing browser...\")\n",
    "            driver.quit()\n",
    "\n",
    "# --- Run the scraper ---\n",
    "final_inventory_data = scrape_export_inventory_data()\n",
    "\n",
    "# --- Display the final DataFrame in the notebook for confirmation ---\n",
    "if final_inventory_data is not None and not final_inventory_data.empty:\n",
    "    print(\"\\n\\n--- FINAL DATA PREVIEW ---\")\n",
    "    display(final_inventory_data) \n",
    "else:\n",
    "    print(\"\\n\\n--- TASK FAILED OR NO DATA FOUND ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6421aa",
   "metadata": {},
   "source": [
    "Here is a breakdown of the entire code, explaining its purpose and functionality step-by-step.\n",
    "High-Level Goal\n",
    "The script's main objective is to automate the following process:\n",
    "Go to a Chinese coal industry website.\n",
    "Find all news articles related to \"coal inventory.\"\n",
    "Visit each of those articles and extract specific data points: the date, the port name, and the inventory level.\n",
    "Organize this scattered data into a clean, structured table.\n",
    "Export this final table into a CSV file that can be easily opened in Excel or other data analysis tools.\n",
    "Detailed Code Explanation\n",
    "1. Imports and Setup\n",
    "The script begins by importing necessary libraries. It includes a clever try...except block to ensure the required libraries are installed before use.\n",
    "pandas as pd: A powerful library for data manipulation and analysis. It's used here to create and reshape the data into a final table (a DataFrame).\n",
    "re: Python's built-in regular expression library, used for finding and extracting text that matches a specific pattern.\n",
    "datetime: Used to handle and format dates correctly.\n",
    "time: Used to pause the script execution (time.sleep()).\n",
    "undetected_chromedriver as uc: A special version of the Selenium browser driver designed to be less detectable by websites that try to block automated scripts (bots).\n",
    "selenium: The core library for browser automation. It allows the script to programmatically control a Chrome browser to navigate pages, find elements, and extract content.\n",
    "2. The scrape_export_inventory_data() Function\n",
    "This is the main function that contains all the logic.\n",
    "search_url: The starting point for the scraper—a pre-defined search results page for \"煤炭库存\" (coal inventory).\n",
    "csv_filename: The name of the file where the final data will be saved.\n",
    "Browser Initialization:\n",
    "It sets up Chrome to run in --headless mode, meaning no visible browser window will pop up. The entire process runs in the background.\n",
    "driver = uc.Chrome(...) launches the automated browser.\n",
    "wait = WebDriverWait(driver, 25) is an important tool for reliability. It tells Selenium to wait up to 25 seconds for certain conditions to be met before throwing an error, which prevents the script from failing on slow-loading pages.\n",
    "driver.get(search_url): The browser navigates to the search results page.\n",
    "driver.find_elements(By.PARTIAL_LINK_TEXT, \"煤炭库存\"): The script searches for all hyperlink elements (<a> tags) on the page whose visible text contains \"煤炭库存\". This is how it identifies the relevant articles.\n",
    "article_urls = sorted(list(set([...]))): This line cleverly processes the found links.\n",
    "It gets the href (the URL) from each link element.\n",
    "It uses set() to automatically remove any duplicate URLs.\n",
    "It converts the set back to a list and sorts it for consistent processing order.\n",
    "The script now iterates through each unique URL it found.\n",
    "driver.get(url): It visits each article page one by one.\n",
    "time.sleep(3): It pauses for 3 seconds to allow dynamic content on the page to finish loading.\n",
    "page_text = driver.find_element(By.TAG_NAME, 'body').text: It grabs all the visible text from the page.\n",
    "Pattern Matching (Regular Expression):\n",
    "pattern = re.compile(r\"(\\d+月\\d+日)，([\\u4e00-\\u9fa5]+港)煤炭库存为(\\d+\\.?\\d*)\"): This is the core of the data extraction. It looks for a very specific text pattern:\n",
    "(\\d+月\\d+日): A date like \"8月23日\".\n",
    "([\\u4e00-\\u9fa5]+港): A port name, consisting of Chinese characters ending with \"港\" (port).\n",
    "煤炭库存为: The literal text \"coal inventory is\".\n",
    "(\\d+\\.?\\d*): A number, which can be an integer or a decimal.\n",
    "match = pattern.search(page_text): It searches the page text for this pattern.\n",
    "If a match is found, it extracts the captured groups (date, port, inventory), stores them in a dictionary, and adds it to the raw_data_list.\n",
    "After collecting all the raw data, it needs to be cleaned and structured.\n",
    "long_df = pd.DataFrame(raw_data_list): Converts the list of dictionaries into a \"long-format\" DataFrame, where each row is a single observation.\n",
    "Date Conversion: It creates a proper FullDate column by combining the scraped \"Month-Day\" with the current year.\n",
    "pivot_table: This is the key transformation. It reshapes the data from a long format to a \"wide format\" table, which is much more readable.\n",
    "index='FullDate': The rows of the new table will be the unique dates.\n",
    "columns='Port': The columns will be the unique port names.\n",
    "values='Inventory': The cells of the table will be filled with the inventory values.\n",
    "The script then sorts the table by date and formats the date column into a more standard mm-dd-yy string format.\n",
    "final_table.to_csv(csv_filename, index=False, encoding='utf-8-sig'): The final, clean DataFrame is saved to a CSV file.\n",
    "index=False: Prevents pandas from writing its internal row numbers (0, 1, 2...) into the file.\n",
    "encoding='utf-8-sig': This is crucial for ensuring that the Chinese characters are readable when the CSV is opened in programs like Microsoft Excel.\n",
    "3. Error Handling and Execution\n",
    "try...except...finally Block: The entire scraping process is wrapped in this block.\n",
    "The except part will catch any critical errors during the script's run and print a helpful message instead of crashing.\n",
    "The finally block ensures that driver.quit() is always called at the end, whether the script succeeded or failed. This is very important for closing the browser process and freeing up system resources.\n",
    "Running the Script:\n",
    "The code at the very bottom calls the scrape_export_inventory_data() function to start the process.\n",
    "After it finishes, it checks if the returned data is valid and then uses display() to show a preview of the final table directly in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53920a56",
   "metadata": {},
   "source": [
    "# Dynamic Graph scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb510631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-Level EIA Data Categories ---\n",
      "- ID: coal                 Name: Coal\n",
      "- ID: crude-oil-imports    Name: Crude Oil Imports\n",
      "- ID: electricity          Name: Electricity\n",
      "- ID: international        Name: International\n",
      "- ID: natural-gas          Name: Natural Gas\n",
      "- ID: nuclear-outages      Name: Nuclear Outages\n",
      "- ID: petroleum            Name: Petroleum\n",
      "- ID: seds                 Name: State Energy Data System (SEDS)\n",
      "- ID: steo                 Name: Short Term Energy Outlook\n",
      "- ID: densified-biomass    Name: Densified Biomass\n",
      "- ID: total-energy         Name: Total Energy\n",
      "- ID: aeo                  Name: Annual Energy Outlook\n",
      "- ID: ieo                  Name: International Energy Outlook\n",
      "- ID: co2-emissions        Name: State CO2 Emissions- deprecated: see SEDS\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Your hardcoded API key\n",
    "api_key = \"hM4bggzq9uz0AejSnhyETcnGEtNuVxlc6tsVbeiH\" \n",
    "\n",
    "# The base URL for the EIA API version 2\n",
    "base_url = \"https://api.eia.gov/v2/\"\n",
    "\n",
    "params = {'api_key': api_key}\n",
    "\n",
    "try:\n",
    "    # Make the request to the top-level of the API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"--- Top-Level EIA Data Categories ---\")\n",
    "    for route in data['response']['routes']:\n",
    "        print(f\"- ID: {route['id']:<20} Name: {route['name']}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9756970f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Exploring the Coal section at: https://api.eia.gov/v2/coal/\n",
      "\n",
      "--- Available Sub-Categories within Coal ---\n",
      "- ID: shipments                      Name: Coal Shipments\n",
      "- ID: consumption-and-quality        Name: Consumption and Quality\n",
      "- ID: aggregate-production           Name: Aggregate Production\n",
      "- ID: exports-imports-quantity-price Name: Exports\\Imports Quantity\\Price\n",
      "- ID: market-sales-price             Name: Market Sales Price\n",
      "- ID: mine-production                Name: Mine Production\n",
      "- ID: price-by-rank                  Name: Price by Rank\n",
      "- ID: reserves-capacity              Name: Reserves Capacity\n"
     ]
    }
   ],
   "source": [
    "# The 'base_url' and 'params' variables from Cell 1 are still in memory.\n",
    "\n",
    "# Let's look inside the 'coal' category we found\n",
    "coal_url = base_url + \"coal/\" \n",
    "\n",
    "print(f\"--> Exploring the Coal section at: {coal_url}\")\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(coal_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Now, let's print the sub-categories found inside 'coal'\n",
    "print(\"\\n--- Available Sub-Categories within Coal ---\")\n",
    "for route in data['response']['routes']:\n",
    "    print(f\"- ID: {route['id']:<30} Name: {route['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a56a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exploring ALL routes and data endpoints under 'coal/' ---\n",
      "- Route: shipments (Name: Coal Shipments)\n",
      "  - Route: mine-state-aggregates (Name: Mine State Aggregates)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - mineStateId (Mine State\\Region)\n",
      "        - coalRankId (Coal Rank)\n",
      "  - Route: receipts (Name: Receipts)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - plantStateId (Plant State\\Region)\n",
      "        - mineStateId (Mine State\\Region)\n",
      "        - mineTypeId (Mine Type)\n",
      "        - mineMSHAId (Mine)\n",
      "        - mineBasinId (Mine Basin)\n",
      "        - mineCountyId (Mine County)\n",
      "        - contractType (Contract Type)\n",
      "        - transportationMode (Transportation Mode)\n",
      "        - coalSupplier (Coal Supplier)\n",
      "        - coalRankId (Coal Rank)\n",
      "        - plantId (Plant)\n",
      "  - Route: mine-aggregates (Name: Mine Aggregates)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - mineStateId (Mine State\\Region)\n",
      "        - mineMSHAId (Mine)\n",
      "        - coalRankId (Coal Rank)\n",
      "  - Route: plant-state-aggregates (Name: Plant State Aggregates)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - plantStateId (Plant State\\Region)\n",
      "        - coalRankId (Coal Rank)\n",
      "  - Route: plant-aggregates (Name: Plant Aggregates)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - location (Plant State\\Region.)\n",
      "        - plant (Plant)\n",
      "        - rank (Coal Rank)\n",
      "  - Route: by-mine-by-plant (Name: By Mine, By Plant)\n",
      "      ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "        - mineStateId (Mine State\\Region)\n",
      "        - mineMSHAId (Mine)\n",
      "        - coalSupplier (Coal Supplier)\n",
      "        - plantStateId (Plant State\\Region)\n",
      "        - plantId (Plant)\n",
      "        - coalType (Coal Type)\n",
      "- Route: consumption-and-quality (Name: Consumption and Quality)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - location (U.S. Total, States, and Census Regions.)\n",
      "      - sector (Sector)\n",
      "- Route: aggregate-production (Name: Aggregate Production)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - stateRegionId (State\\Region)\n",
      "      - coalRankId (Coal Rank)\n",
      "      - mineTypeId (Mine Type)\n",
      "- Route: exports-imports-quantity-price (Name: Exports\\Imports Quantity\\Price)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - exportImportType (Export\\Import Type)\n",
      "      - coalRankId (Coal Rank)\n",
      "      - countryId (Country)\n",
      "      - customsDistrictId (Customs District)\n",
      "- Route: market-sales-price (Name: Market Sales Price)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - stateRegionId (State\\Region)\n",
      "      - marketTypeId (Market Type)\n",
      "- Route: mine-production (Name: Mine Production)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - mineMSHAId (Mine)\n",
      "      - stateId (State)\n",
      "      - regionId (Region)\n",
      "      - supplyRegionId (Supply Region)\n",
      "      - censusRegionId (Census Region)\n",
      "      - mississippiRegionId (Mississippi Region)\n",
      "      - mineCountyId (Mine County)\n",
      "      - coalRankId (Coal Rank)\n",
      "      - mineTypeId (Mine Type)\n",
      "      - mineStatusId (Mine Status)\n",
      "- Route: price-by-rank (Name: Price by Rank)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - stateRegionId (State\\Region)\n",
      "      - coalRankId (Coal Rank)\n",
      "- Route: reserves-capacity (Name: Reserves Capacity)\n",
      "    ↳ This is a DATA ENDPOINT. Available Filters:\n",
      "      - stateId (State\\Region)\n",
      "      - mineTypeId (Mine Type)\n",
      "\n",
      "--- Exploration Complete ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your API key. For real projects, always use environment variables.\n",
    "API_KEY = \"hM4bggzq9uz0AejSnhyETcnGEtNuVxlc6tsVbeiH\" \n",
    "BASE_URL = \"https://api.eia.gov/v2/\"\n",
    "# The section of the API you want to explore.\n",
    "STARTING_ROUTE = \"coal/\"\n",
    "\n",
    "def explore_api_route(route, indent=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively explores a route in the EIA API, printing its sub-routes\n",
    "    and any available data filters (facets) found at that exact level.\n",
    "    This provides a complete map of the API section.\n",
    "\n",
    "    Args:\n",
    "        route (str): The API route to explore (e.g., \"coal/\").\n",
    "        indent (str): A string of spaces used for formatting the output tree.\n",
    "    \"\"\"\n",
    "    # Construct the full URL for the current route\n",
    "    url = BASE_URL + route\n",
    "    params = {'api_key': API_KEY}\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params)\n",
    "        # If the URL is invalid, just return and stop exploring this branch\n",
    "        if response.status_code != 200:\n",
    "            return \n",
    "        \n",
    "        data = response.json()\n",
    "        response_data = data.get('response', {})\n",
    "\n",
    "        # Check for data filters (facets) at the current level\n",
    "        if 'facets' in response_data and response_data['facets']:\n",
    "            print(f\"{indent}  ↳ This is a DATA ENDPOINT. Available Filters:\")\n",
    "            for facet in response_data['facets']:\n",
    "                print(f\"{indent}    - {facet['id']} ({facet.get('description', 'No description')})\")\n",
    "\n",
    "        # Check for sub-routes to explore deeper\n",
    "        if 'routes' in response_data and response_data['routes']:\n",
    "            sub_routes = response_data['routes']\n",
    "            for sub_route in sub_routes:\n",
    "                # Print the current sub-route with indentation\n",
    "                print(f\"{indent}- Route: {sub_route['id']} (Name: {sub_route['name']})\")\n",
    "                # Recursively call this function to explore the sub-route\n",
    "                explore_api_route(f\"{route}{sub_route['id']}/\", indent + \"  \")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"{indent}  [Error exploring {route}: {e}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"{indent}  [An unexpected error occurred at {route}: {e}]\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Exploring ALL routes and data endpoints under '{STARTING_ROUTE}' ---\")\n",
    "    explore_api_route(STARTING_ROUTE)\n",
    "    print(\"\\n--- Exploration Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bb964",
   "metadata": {},
   "source": [
    "# Information about the API\n",
    "\n",
    "2. Decoding the Series ID Pattern (The \"Manual\" Way)\n",
    "The older series_ids, like the one we used, often follow a logical pattern. Once you learn it, you can sometimes guess or construct the ID you need.\n",
    "\n",
    "    Let's break down COAL.PRODUCTION.US-TOT.A:\n",
    "\n",
    "    COAL: The energy source.\n",
    "\n",
    "    PRODUCTION: The type of data (as opposed to PRICE, CONSUMPTION, etc.).\n",
    "\n",
    "    US: The geographic area (United States total).\n",
    "\n",
    "    TOT: A sub-filter, in this case, \"Total\" of all coal ranks.\n",
    "\n",
    "    A: The frequency of the data (A for Annual, M for Monthly, Q for Quarterly, W for Weekly).\n",
    "\n",
    "    So, by looking at the ID, we can tell it represents the Total U.S. Annual Coal Production. While this is useful, using the Data Browser is always the most reliable way to confirm the correct and most up-to-date ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d976658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Requesting monthly data for COAL.PRODUCTION.US-TOT.M from 2022 to 2023...\n",
      "\n",
      "--- SUCCESS! Found 23 records. Printing them now: ---\n",
      "{'period': '2023-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 577954017, 'production-units': 'short tons'}\n",
      "{'period': '2022-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 594155282, 'production-units': 'short tons'}\n",
      "{'period': '2021-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 577431278, 'production-units': 'short tons'}\n",
      "{'period': '2020-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 535434354, 'production-units': 'short tons'}\n",
      "{'period': '2019-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 706309263, 'production-units': 'short tons'}\n",
      "{'period': '2018-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 756167095, 'production-units': 'short tons'}\n",
      "{'period': '2017-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 774609357, 'production-units': 'short tons'}\n",
      "{'period': '2016-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 728364498, 'production-units': 'short tons'}\n",
      "{'period': '2015-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 896940563, 'production-units': 'short tons'}\n",
      "{'period': '2014-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1000048758, 'production-units': 'short tons'}\n",
      "{'period': '2013-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 984841779, 'production-units': 'short tons'}\n",
      "{'period': '2012-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1016458418, 'production-units': 'short tons'}\n",
      "{'period': '2011-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1095627536, 'production-units': 'short tons'}\n",
      "{'period': '2010-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1084368148, 'production-units': 'short tons'}\n",
      "{'period': '2009-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1074923392, 'production-units': 'short tons'}\n",
      "{'period': '2008-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1171808669, 'production-units': 'short tons'}\n",
      "{'period': '2007-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1146635345, 'production-units': 'short tons'}\n",
      "{'period': '2006-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1162749659, 'production-units': 'short tons'}\n",
      "{'period': '2005-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1131498099, 'production-units': 'short tons'}\n",
      "{'period': '2004-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1112098870, 'production-units': 'short tons'}\n",
      "{'period': '2003-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1071752573, 'production-units': 'short tons'}\n",
      "{'period': '2002-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1094283061, 'production-units': 'short tons'}\n",
      "{'period': '2001-01', 'stateRegionId': 'US', 'stateRegionDescription': 'U.S. Total', 'coalRankId': 'TOT', 'coalRankDescription': 'All', 'mineTypeId': 'ALL', 'mineTypeDescription': 'All', 'production': 1127688806, 'production-units': 'short tons'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# It is best practice to store your API key as an environment variable.\n",
    "# For this example, we will define it directly.\n",
    "api_key = \"hM4bggzq9uz0AejSnhyETcnGEtNuVxlc6tsVbeiH\" \n",
    "base_url = \"https://api.eia.gov/v2/\"\n",
    "\n",
    "# --- Using the direct seriesid route ---\n",
    "# The '.M' at the end requests monthly data.\n",
    "series_id = 'COAL.PRODUCTION.US-TOT.M' \n",
    "data_url = base_url + f\"seriesid/{series_id}\"\n",
    "\n",
    "# To get all months, we must include start and end dates in the request.\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'start': '2022', # Start year\n",
    "    'end': '2023'    # End year\n",
    "}\n",
    "\n",
    "print(f\"--> Requesting monthly data for {series_id} from {params['start']} to {params['end']}...\")\n",
    "\n",
    "# --- Make the API Request ---\n",
    "try:\n",
    "    response = requests.get(data_url, params=params)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    data = response.json()\n",
    "\n",
    "    # --- Print the Raw Data Records ---\n",
    "    data_records = data['response']['data']\n",
    "    \n",
    "    if not data_records:\n",
    "        raise ValueError(\"API returned an empty data list. The requested data may not be available for this period.\")\n",
    "\n",
    "    print(f\"\\n--- SUCCESS! Found {len(data_records)} records. Printing them now: ---\")\n",
    "    \n",
    "    # Loop through each record (which is a dictionary) and print it\n",
    "    for record in data_records:\n",
    "        print(record)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n--> A network error occurred: {e}\")\n",
    "except (KeyError, ValueError) as e:\n",
    "    print(f\"\\n--> An error occurred processing the response: {e}\")\n",
    "    print(\"Full API response:\", data)\n",
    "except Exception as e:\n",
    "    print(f\"\\n--> An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://api.eia.gov/v2/coal/exports-imports-quantity-price/data/?frequency=quarterly&data[0]=price&data[1]=quantity&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e375700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Requesting quarterly export data from: https://api.eia.gov/v2/coal/exports-imports-quantity-price/data/\n",
      "\n",
      "--- SUCCESS! Found 5000 records. Organizing into a table: ---\n",
      "\n",
      "Data has been successfully exported to 'eia_coal_exports.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z.Peng\\AppData\\Local\\Temp\\ipykernel_8112\\80842215.py:44: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['period_dt'] = pd.to_datetime(df['period'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exportImportType</th>\n",
       "      <th>coalRankId</th>\n",
       "      <th>coalRankDescription</th>\n",
       "      <th>countryId</th>\n",
       "      <th>countryDescription</th>\n",
       "      <th>customsDistrictId</th>\n",
       "      <th>customsDistrictDescription</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price-units</th>\n",
       "      <th>quantity-units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q1 2025</th>\n",
       "      <td>Exports</td>\n",
       "      <td>TOT</td>\n",
       "      <td>All</td>\n",
       "      <td>TOT</td>\n",
       "      <td>All; Non-United States</td>\n",
       "      <td>BA_MD</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>96.83</td>\n",
       "      <td>6664251</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1 2025</th>\n",
       "      <td>Exports</td>\n",
       "      <td>STM</td>\n",
       "      <td>Steam Coal</td>\n",
       "      <td>ID</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>SF_CA</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>66.8</td>\n",
       "      <td>66690</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1 2025</th>\n",
       "      <td>Exports</td>\n",
       "      <td>STM</td>\n",
       "      <td>Steam Coal</td>\n",
       "      <td>ID</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>NO_VA</td>\n",
       "      <td>Norfolk, VA</td>\n",
       "      <td>w</td>\n",
       "      <td>169</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1 2025</th>\n",
       "      <td>Exports</td>\n",
       "      <td>STM</td>\n",
       "      <td>Steam Coal</td>\n",
       "      <td>ID</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>NY_NY</td>\n",
       "      <td>New York City, NY</td>\n",
       "      <td>1043</td>\n",
       "      <td>4</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1 2025</th>\n",
       "      <td>Exports</td>\n",
       "      <td>STM</td>\n",
       "      <td>Steam Coal</td>\n",
       "      <td>IN</td>\n",
       "      <td>India</td>\n",
       "      <td>TOT</td>\n",
       "      <td>Total</td>\n",
       "      <td>79.68</td>\n",
       "      <td>4157855</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4 2023</th>\n",
       "      <td>Exports</td>\n",
       "      <td>MET</td>\n",
       "      <td>Metallurgical</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>MO_AL</td>\n",
       "      <td>Mobile, AL</td>\n",
       "      <td>w</td>\n",
       "      <td>63762</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4 2023</th>\n",
       "      <td>Exports</td>\n",
       "      <td>MET</td>\n",
       "      <td>Metallurgical</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "      <td>BA_MD</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>247.5</td>\n",
       "      <td>686372</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4 2023</th>\n",
       "      <td>Exports</td>\n",
       "      <td>MET</td>\n",
       "      <td>Metallurgical</td>\n",
       "      <td>IT</td>\n",
       "      <td>Italy</td>\n",
       "      <td>TOT</td>\n",
       "      <td>Total</td>\n",
       "      <td>207.96</td>\n",
       "      <td>289573</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4 2023</th>\n",
       "      <td>Exports</td>\n",
       "      <td>MET</td>\n",
       "      <td>Metallurgical</td>\n",
       "      <td>IT</td>\n",
       "      <td>Italy</td>\n",
       "      <td>NO_VA</td>\n",
       "      <td>Norfolk, VA</td>\n",
       "      <td>w</td>\n",
       "      <td>289573</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4 2023</th>\n",
       "      <td>Exports</td>\n",
       "      <td>MET</td>\n",
       "      <td>Metallurgical</td>\n",
       "      <td>IT</td>\n",
       "      <td>Italy</td>\n",
       "      <td>NO_LA</td>\n",
       "      <td>New Orleans, LA</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per short ton</td>\n",
       "      <td>short tons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        exportImportType coalRankId coalRankDescription countryId  \\\n",
       "period                                                              \n",
       "Q1 2025          Exports        TOT                 All       TOT   \n",
       "Q1 2025          Exports        STM          Steam Coal        ID   \n",
       "Q1 2025          Exports        STM          Steam Coal        ID   \n",
       "Q1 2025          Exports        STM          Steam Coal        ID   \n",
       "Q1 2025          Exports        STM          Steam Coal        IN   \n",
       "...                  ...        ...                 ...       ...   \n",
       "Q4 2023          Exports        MET       Metallurgical        JP   \n",
       "Q4 2023          Exports        MET       Metallurgical        JP   \n",
       "Q4 2023          Exports        MET       Metallurgical        IT   \n",
       "Q4 2023          Exports        MET       Metallurgical        IT   \n",
       "Q4 2023          Exports        MET       Metallurgical        IT   \n",
       "\n",
       "             countryDescription customsDistrictId customsDistrictDescription  \\\n",
       "period                                                                         \n",
       "Q1 2025  All; Non-United States             BA_MD              Baltimore, MD   \n",
       "Q1 2025               Indonesia             SF_CA          San Francisco, CA   \n",
       "Q1 2025               Indonesia             NO_VA                Norfolk, VA   \n",
       "Q1 2025               Indonesia             NY_NY          New York City, NY   \n",
       "Q1 2025                   India               TOT                      Total   \n",
       "...                         ...               ...                        ...   \n",
       "Q4 2023                   Japan             MO_AL                 Mobile, AL   \n",
       "Q4 2023                   Japan             BA_MD              Baltimore, MD   \n",
       "Q4 2023                   Italy               TOT                      Total   \n",
       "Q4 2023                   Italy             NO_VA                Norfolk, VA   \n",
       "Q4 2023                   Italy             NO_LA            New Orleans, LA   \n",
       "\n",
       "          price quantity            price-units quantity-units  \n",
       "period                                                          \n",
       "Q1 2025   96.83  6664251  dollars per short ton     short tons  \n",
       "Q1 2025    66.8    66690  dollars per short ton     short tons  \n",
       "Q1 2025       w      169  dollars per short ton     short tons  \n",
       "Q1 2025    1043        4  dollars per short ton     short tons  \n",
       "Q1 2025   79.68  4157855  dollars per short ton     short tons  \n",
       "...         ...      ...                    ...            ...  \n",
       "Q4 2023       w    63762  dollars per short ton     short tons  \n",
       "Q4 2023   247.5   686372  dollars per short ton     short tons  \n",
       "Q4 2023  207.96   289573  dollars per short ton     short tons  \n",
       "Q4 2023       w   289573  dollars per short ton     short tons  \n",
       "Q4 2023    None        0  dollars per short ton     short tons  \n",
       "\n",
       "[5000 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your API key.\n",
    "api_key = \"hM4bggzq9uz0AejSnhyETcnGEtNuVxlc6tsVbeiH\" \n",
    "\n",
    "# The exact URL for the data endpoint\n",
    "data_url = \"https://api.eia.gov/v2/coal/exports-imports-quantity-price/data/\"\n",
    "\n",
    "# The parameters from the URL you provided\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'frequency': 'quarterly',\n",
    "    'data[0]': 'price',\n",
    "    'data[1]': 'quantity',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000 # Max records per request\n",
    "}\n",
    "\n",
    "print(f\"--> Requesting quarterly export data from: {data_url}\")\n",
    "\n",
    "# --- Make the API Request ---\n",
    "try:\n",
    "    response = requests.get(data_url, params=params)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    data = response.json()\n",
    "\n",
    "    # --- Load and Organize the data into a pandas DataFrame ---\n",
    "    data_records = data['response']['data']\n",
    "    \n",
    "    if not data_records:\n",
    "        raise ValueError(\"API returned an empty data list for this query.\")\n",
    "\n",
    "    print(f\"\\n--- SUCCESS! Found {len(data_records)} records. Organizing into a table: ---\")\n",
    "    \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Convert the 'period' column to datetime objects for proper sorting\n",
    "    df['period_dt'] = pd.to_datetime(df['period'])\n",
    "    \n",
    "    # Create a new column with the desired \"Quarter Year\" format\n",
    "    df['period'] = 'Q' + df['period_dt'].dt.quarter.astype(str) + ' ' + df['period_dt'].dt.year.astype(str)\n",
    "    \n",
    "    # Set the new formatted 'period' column as the index of the table\n",
    "    df.set_index('period', inplace=True)\n",
    "    \n",
    "    # Remove the temporary datetime column as it's no longer needed\n",
    "    df.drop(columns=['period_dt'], inplace=True)\n",
    "    \n",
    "    # --- Export the DataFrame to a CSV file ---\n",
    "    output_filename = 'eia_coal_exports.csv'\n",
    "    df.to_csv(output_filename)\n",
    "    print(f\"\\nData has been successfully exported to '{output_filename}'\")\n",
    "\n",
    "    # Display the organized DataFrame as a clean table\n",
    "    display(df)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n--> A network error occurred: {e}\")\n",
    "except (KeyError, ValueError) as e:\n",
    "    print(f\"\\n--> An error occurred processing the response: {e}\")\n",
    "    print(\"Full API response:\", data)\n",
    "except Exception as e:\n",
    "    print(f\"\\n--> An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Requesting raw monthly export data from: https://api.eia.gov/v2/coal/exports-imports-quantity-price/data/\n",
      "\n",
      "--> A network error occurred: 500 Server Error: Internal Server Error for url: https://api.eia.gov/v2/coal/exports-imports-quantity-price/data/?api_key=hM4bggzq9uz0AejSnhyETcnGEtNuVxlc6tsVbeiH&frequency=monthly&data%5B0%5D=quantity&facets%5BexportImportType%5D%5B%5D=E&sort%5B0%5D%5Bcolumn%5D=period&sort%5B0%5D%5Bdirection%5D=desc&start=2022&end=2023&length=5000\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
