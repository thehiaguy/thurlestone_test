{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1c0209",
   "metadata": {},
   "source": [
    "# Coal Inventory Scraper for Chinese Ports\n",
    "\n",
    "This notebook contains a Python script that scrapes coal inventory data from the sxcoal.com website. It specifically targets articles containing \"煤炭库存\" (coal inventory) to extract date, port name, and inventory figures. The extracted data is then reshaped into a table and saved as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4081493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing browser for scrape-and-export task ---\n",
      "Navigating to: https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\n",
      "Navigating to: https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\n",
      "\n",
      "Finding all relevant inventory articles...\n",
      ">>> Success! Found 16 unique articles to process.\n",
      "\n",
      "--- Processing article 1 of 16 ---\n",
      "\n",
      "Finding all relevant inventory articles...\n",
      ">>> Success! Found 16 unique articles to process.\n",
      "\n",
      "--- Processing article 1 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 2 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 2 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '黄骅港' with inventory 175.6\n",
      "\n",
      "--- Processing article 3 of 16 ---\n",
      "  >>> SUCCESS: Found [7月2日] '黄骅港' with inventory 175.6\n",
      "\n",
      "--- Processing article 3 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 4 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '秦皇岛港' with inventory 580\n",
      "\n",
      "--- Processing article 4 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '黄骅港' with inventory 180.3\n",
      "\n",
      "--- Processing article 5 of 16 ---\n",
      "  >>> SUCCESS: Found [7月3日] '黄骅港' with inventory 180.3\n",
      "\n",
      "--- Processing article 5 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 6 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 6 of 16 ---\n",
      "  >>> SUCCESS: Found [7月4日] '秦皇岛港' with inventory 570\n",
      "\n",
      "--- Processing article 7 of 16 ---\n",
      "  >>> SUCCESS: Found [7月4日] '秦皇岛港' with inventory 570\n",
      "\n",
      "--- Processing article 7 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 8 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 8 of 16 ---\n",
      "  >>> SUCCESS: Found [7月7日] '秦皇岛港' with inventory 576\n",
      "\n",
      "--- Processing article 9 of 16 ---\n",
      "  >>> SUCCESS: Found [7月7日] '秦皇岛港' with inventory 576\n",
      "\n",
      "--- Processing article 9 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '秦皇岛港' with inventory 575\n",
      "\n",
      "--- Processing article 10 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '秦皇岛港' with inventory 575\n",
      "\n",
      "--- Processing article 10 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '黄骅港' with inventory 182.5\n",
      "\n",
      "--- Processing article 11 of 16 ---\n",
      "  >>> SUCCESS: Found [7月8日] '黄骅港' with inventory 182.5\n",
      "\n",
      "--- Processing article 11 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 12 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Processing article 12 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '秦皇岛港' with inventory 573\n",
      "\n",
      "--- Processing article 13 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '秦皇岛港' with inventory 573\n",
      "\n",
      "--- Processing article 13 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '黄骅港' with inventory 189.3\n",
      "\n",
      "--- Processing article 14 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '黄骅港' with inventory 189.3\n",
      "\n",
      "--- Processing article 14 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '京唐港' with inventory 701\n",
      "\n",
      "--- Processing article 15 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '京唐港' with inventory 701\n",
      "\n",
      "--- Processing article 15 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '曹妃甸港' with inventory 1254\n",
      "\n",
      "--- Processing article 16 of 16 ---\n",
      "  >>> SUCCESS: Found [7月9日] '曹妃甸港' with inventory 1254\n",
      "\n",
      "--- Processing article 16 of 16 ---\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Data extraction complete. Reshaping table... ---\n",
      "\n",
      "--- Reshaping complete. Exporting data to 'coal_inventory_data.csv'... ---\n",
      ">>> SUCCESS: Data has been saved to coal_inventory_data.csv\n",
      "\n",
      "Script finished. Closing browser...\n",
      "  > INFO: No data matching the required pattern was found.\n",
      "\n",
      "--- Data extraction complete. Reshaping table... ---\n",
      "\n",
      "--- Reshaping complete. Exporting data to 'coal_inventory_data.csv'... ---\n",
      ">>> SUCCESS: Data has been saved to coal_inventory_data.csv\n",
      "\n",
      "Script finished. Closing browser...\n",
      "\n",
      "\n",
      "--- FINAL DATA PREVIEW ---\n",
      "\n",
      "\n",
      "--- FINAL DATA PREVIEW ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Port</th>\n",
       "      <th>Date</th>\n",
       "      <th>京唐港</th>\n",
       "      <th>曹妃甸港</th>\n",
       "      <th>秦皇岛港</th>\n",
       "      <th>黄骅港</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-09-25</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>189.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-08-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575.0</td>\n",
       "      <td>182.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-07-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>576.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-04-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580.0</td>\n",
       "      <td>180.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580.0</td>\n",
       "      <td>175.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Port      Date    京唐港    曹妃甸港   秦皇岛港    黄骅港\n",
       "0     07-09-25  701.0  1254.0  573.0  189.3\n",
       "1     07-08-25    NaN     NaN  575.0  182.5\n",
       "2     07-07-25    NaN     NaN  576.0    NaN\n",
       "3     07-04-25    NaN     NaN  570.0    NaN\n",
       "4     07-03-25    NaN     NaN  580.0  180.3\n",
       "5     07-02-25    NaN     NaN  580.0  175.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## need to install undetected_chromedriver and pandas if not already installed\n",
    "# This script scrapes coal inventory data from a specific website, reshapes it, and exports\n",
    "# the final table to a CSV file. It uses Selenium for web scraping and Pandas for data manipulation.\n",
    "# File: coal_inventory_scraper.py\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "except ImportError:\n",
    "    print(\"Required libraries not found. Installing them now...\")\n",
    "    %pip install undetected-chromedriver pandas\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_export_inventory_data():\n",
    "    \"\"\"\n",
    "    Final version: Scrapes all inventory articles, reshapes the data,\n",
    "    and exports the final table to a CSV file.\n",
    "    \"\"\"\n",
    "    search_url = \"https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\"\n",
    "    csv_filename = \"coal_inventory_data.csv\"\n",
    "    raw_data_list = []\n",
    "\n",
    "    print(\"--- Initializing browser for scrape-and-export task ---\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "        driver = uc.Chrome(options=options, use_subprocess=False)\n",
    "        wait = WebDriverWait(driver, 25) \n",
    "\n",
    "        # Step 1: Gather URLs\n",
    "        print(f\"Navigating to: {search_url}\")\n",
    "        driver.get(search_url)\n",
    "\n",
    "        print(\"\\nFinding all relevant inventory articles...\")\n",
    "        article_link_selector = (By.PARTIAL_LINK_TEXT, \"煤炭库存\")\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable(article_link_selector))\n",
    "            article_elements = driver.find_elements(*article_link_selector)\n",
    "            article_urls = sorted(list(set([el.get_attribute('href') for el in article_elements])))\n",
    "            print(f\">>> Success! Found {len(article_urls)} unique articles to process.\")\n",
    "        except TimeoutException:\n",
    "            print(\"\\n--- FAILURE ---\")\n",
    "            print(\"Could not find any article links containing '煤炭库存'.\")\n",
    "            return None \n",
    "\n",
    "        # Step 2: Loop through URLs and extract data\n",
    "        for i, url in enumerate(article_urls):\n",
    "            print(f\"\\n--- Processing article {i+1} of {len(article_urls)} ---\")\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(3) \n",
    "                page_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "                pattern = re.compile(r\"(\\d+月\\d+日)，([\\u4e00-\\u9fa5]+港)煤炭库存为(\\d+\\.?\\d*)\")\n",
    "                match = pattern.search(page_text)\n",
    "                \n",
    "                if match:\n",
    "                    date_str, port_name, inventory = match.groups()\n",
    "                    raw_data_list.append({\"Date\": date_str, \"Port\": port_name.strip(), \"Inventory\": float(inventory)})\n",
    "                    print(f\"  >>> SUCCESS: Found [{date_str}] '{port_name.strip()}' with inventory {inventory}\")\n",
    "                else:\n",
    "                    print(f\"  > INFO: No data matching the required pattern was found.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  > ERROR: An error occurred processing this article: {type(e).__name__}\")\n",
    "        \n",
    "        if not raw_data_list:\n",
    "            print(\"\\nScraping finished, but no data could be extracted.\")\n",
    "            return None\n",
    "            \n",
    "        # Step 3: Reshape the data\n",
    "        print(\"\\n--- Data extraction complete. Reshaping table... ---\")\n",
    "        long_df = pd.DataFrame(raw_data_list)\n",
    "        current_year = datetime.now().year\n",
    "        long_df['FullDate'] = long_df['Date'].apply(lambda x: datetime.strptime(f'{current_year}年{x}', '%Y年%m月%d日'))\n",
    "\n",
    "        final_table = long_df.pivot_table(index='FullDate', columns='Port', values='Inventory')\n",
    "        final_table.sort_index(ascending=False, inplace=True)\n",
    "        final_table.reset_index(inplace=True)\n",
    "        final_table.rename(columns={'FullDate': 'Date'}, inplace=True)\n",
    "        final_table['Date'] = final_table['Date'].dt.strftime('%m-%d-%y')\n",
    "        \n",
    "        # ***4: EXPORT TO CSV ***\n",
    "        print(f\"\\n--- Reshaping complete. Exporting data to '{csv_filename}'... ---\")\n",
    "        try:\n",
    "            # We use index=False to avoid writing the pandas row numbers (0, 1, 2...) into the file.\n",
    "            # encoding='utf-8-sig' helps Excel open the file correctly with Chinese characters.\n",
    "            final_table.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\">>> SUCCESS: Data has been saved to {csv_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"--- FAILED TO SAVE FILE: An error occurred: {e} ---\")\n",
    "\n",
    "        return final_table\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nA critical error occurred: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"\\nScript finished. Closing browser...\")\n",
    "            driver.quit()\n",
    "\n",
    "# --- Run the scraper ---\n",
    "final_inventory_data = scrape_export_inventory_data()\n",
    "\n",
    "# --- Display the final DataFrame in the notebook for confirmation ---\n",
    "if final_inventory_data is not None and not final_inventory_data.empty:\n",
    "    print(\"\\n\\n--- FINAL DATA PREVIEW ---\")\n",
    "    display(final_inventory_data) \n",
    "else:\n",
    "    print(\"\\n\\n--- TASK FAILED OR NO DATA FOUND ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
