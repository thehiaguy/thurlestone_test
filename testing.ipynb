{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e3294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "--- Getting historical data for May 15, 2024 ---\n",
      "--- Initializing automated browser ---\n",
      "Navigating to the main page to authenticate...\n",
      "Waiting for security challenges to complete...\n",
      "Executing API call for 20240515 from within the browser...\n",
      "An error occurred: JavaScript execution failed: Error: API request failed with status: 404 \n",
      "Browser has been closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_cme_settlements_final_corrected(trade_date: str = None):\n",
    "    \"\"\"\n",
    "    Scrapes CME settlement data using a pure Selenium approach with the corrected v1 API endpoint.\n",
    "    This method should be robust against all anti-scraping measures.\n",
    "\n",
    "    Args:\n",
    "        trade_date (str, optional): Date in 'YYYYMMDD' format. Defaults to today.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with settlement data, or empty DataFrame on failure.\n",
    "    \"\"\"\n",
    "    main_page_url = \"https://www.cmegroup.com/markets/energy/refined-products/singapore-fob-marine-fuel-05-platts.settlements.html\"\n",
    "    \n",
    "    if not trade_date:\n",
    "        trade_date = datetime.today().strftime('%Y%m%d')\n",
    "        \n",
    "    # The corrected API URL with '/v1/'\n",
    "    # The product ID 'S5F' is found by inspecting the network traffic on the page. 4286 was the old numeric ID.\n",
    "    api_url = f\"https://www.cmegroup.com/CmeWS/mvc/v1/settlements/Future/S5F/FUT?tradeDate={trade_date}&strategy=DEFAULT\"\n",
    "\n",
    "    print(\"--- Initializing automated browser ---\")\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36')\n",
    "    options.add_argument('--log-level=3')\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "        driver.set_script_timeout(45)\n",
    "\n",
    "        print(\"Navigating to the main page to authenticate...\")\n",
    "        driver.get(main_page_url)\n",
    "        \n",
    "        print(\"Waiting for security challenges to complete...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "        print(f\"Executing API call for {trade_date} from within the browser...\")\n",
    "\n",
    "        js_script = \"\"\"\n",
    "            const url = arguments[0];\n",
    "            const callback = arguments[1];\n",
    "            \n",
    "            fetch(url)\n",
    "                .then(response => {\n",
    "                    if (!response.ok) {\n",
    "                        throw new Error('API request failed with status: ' + response.status + ' ' + response.statusText);\n",
    "                    }\n",
    "                    return response.json();\n",
    "                })\n",
    "                .then(data => callback(data))\n",
    "                .catch(error => callback({ 'error': error.toString() }));\n",
    "        \"\"\"\n",
    "\n",
    "        api_response = driver.execute_async_script(js_script, api_url)\n",
    "        \n",
    "        if 'error' in api_response:\n",
    "            raise Exception(f\"JavaScript execution failed: {api_response['error']}\")\n",
    "\n",
    "        print(\"Successfully received data from the API.\")\n",
    "        \n",
    "        settlements_data = api_response.get('settlements', [])\n",
    "\n",
    "        if not settlements_data:\n",
    "            print(f\"No settlement data found for {api_response.get('tradeDate', 'the specified date')}.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(settlements_data)\n",
    "        # The 'priorSettle' key seems to have been renamed to 'priorSettlePrice' in the new API\n",
    "        if 'priorSettlePrice' in df.columns:\n",
    "            df.rename(columns={'priorSettlePrice': 'priorSettle'}, inplace=True)\n",
    "            \n",
    "        print(f\"Successfully created DataFrame for trade date: {api_response.get('tradeDate')}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"Browser has been closed.\")\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"--- Getting historical data for May 15, 2024 ---\")\n",
    "    historical_df = get_cme_settlements_final_corrected(trade_date='20240515')\n",
    "    if not historical_df.empty:\n",
    "        print(\"\\nHistorical Data for 2024-05-15:\")\n",
    "        print(historical_df[['month', 'open', 'high', 'low', 'settle', 'volume', 'openInterest']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4081493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing browser for multi-article scraping ---\n",
      "Navigating to: https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\n",
      "\n",
      "Waiting for inventory articles to appear...\n",
      ">>> Success! Found 15 unique articles to scrape.\n",
      "\n",
      "--- Scraping article 1 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1939511707756490753\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 578.0\n",
      "\n",
      "--- Scraping article 2 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1939874095525621762\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 575.0\n",
      "\n",
      "--- Scraping article 3 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1939874096851021826\n",
      "  > INFO: No data matching the pattern was found in this article's text.\n",
      "\n",
      "--- Scraping article 4 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940236483366043649\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 580.0\n",
      "\n",
      "--- Scraping article 5 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940236484091658242\n",
      "  >>> SUCCESS: Found '黄骅港' with inventory 175.6\n",
      "\n",
      "--- Scraping article 6 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940236485240897537\n",
      "  > INFO: No data matching the pattern was found in this article's text.\n",
      "\n",
      "--- Scraping article 7 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940598871302946817\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 580.0\n",
      "\n",
      "--- Scraping article 8 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940598871961452546\n",
      "  >>> SUCCESS: Found '黄骅港' with inventory 180.3\n",
      "\n",
      "--- Scraping article 9 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940598873131663362\n",
      "  > INFO: No data matching the pattern was found in this article's text.\n",
      "\n",
      "--- Scraping article 10 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1940961259196444674\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 570.0\n",
      "\n",
      "--- Scraping article 11 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1941047223060611073\n",
      "  > INFO: No data matching the pattern was found in this article's text.\n",
      "\n",
      "--- Scraping article 12 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1942048422731792385\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 576.0\n",
      "\n",
      "--- Scraping article 13 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1942410812098953218\n",
      "  >>> SUCCESS: Found '秦皇岛港' with inventory 575.0\n",
      "\n",
      "--- Scraping article 14 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1942410812841345026\n",
      "  >>> SUCCESS: Found '黄骅港' with inventory 182.5\n",
      "\n",
      "--- Scraping article 15 of 15 ---\n",
      "URL: https://www.sxcoal.com/news/detail/1942410814019944450\n",
      "  > INFO: No data matching the pattern was found in this article's text.\n",
      "\n",
      "Script finished. Closing browser...\n",
      "\n",
      "\n",
      "--- SCRAPING COMPLETE ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Port</th>\n",
       "      <th>Inventory (10k tons)</th>\n",
       "      <th>Scrape Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>秦皇岛港</td>\n",
       "      <td>578.0</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>秦皇岛港</td>\n",
       "      <td>575.0</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>秦皇岛港</td>\n",
       "      <td>580.0</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>黄骅港</td>\n",
       "      <td>175.6</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>黄骅港</td>\n",
       "      <td>180.3</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>秦皇岛港</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>秦皇岛港</td>\n",
       "      <td>576.0</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>黄骅港</td>\n",
       "      <td>182.5</td>\n",
       "      <td>2025-07-08 15:04:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Port  Inventory (10k tons)     Scrape Timestamp\n",
       "0  秦皇岛港                 578.0  2025-07-08 15:04:28\n",
       "1  秦皇岛港                 575.0  2025-07-08 15:04:28\n",
       "2  秦皇岛港                 580.0  2025-07-08 15:04:28\n",
       "3   黄骅港                 175.6  2025-07-08 15:04:28\n",
       "5   黄骅港                 180.3  2025-07-08 15:04:28\n",
       "6  秦皇岛港                 570.0  2025-07-08 15:04:28\n",
       "7  秦皇岛港                 576.0  2025-07-08 15:04:28\n",
       "9   黄骅港                 182.5  2025-07-08 15:04:28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "except ImportError:\n",
    "    print(\"Required libraries not found. Installing them now...\")\n",
    "    %pip install undetected-chromedriver pandas\n",
    "    import undetected_chromedriver as uc\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_all_inventory_data_final():\n",
    "    \"\"\"\n",
    "    Final, robust version. It finds all articles, navigates to them, and grabs\n",
    "    all visible text from the page to ensure the data is found regardless of page structure.\n",
    "    \"\"\"\n",
    "    search_url = \"https://www.sxcoal.com/news/search?search=%E7%85%A4%E7%82%AD%E5%BA%93%E5%AD%98\"\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    print(\"--- Initializing browser for multi-article scraping ---\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        # Add '--headless' to hide the browser window after you confirm it's working.\n",
    "        # options.add_argument('--headless')\n",
    "        \n",
    "        driver = uc.Chrome(options=options, use_subprocess=False)\n",
    "        wait = WebDriverWait(driver, 20) \n",
    "\n",
    "        # --- Step 1: Gather all URLs (This part is already working well) ---\n",
    "        print(f\"Navigating to: {search_url}\")\n",
    "        driver.get(search_url)\n",
    "\n",
    "        print(\"\\nWaiting for inventory articles to appear...\")\n",
    "        article_link_selector = (By.PARTIAL_LINK_TEXT, \"煤炭库存\")\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable(article_link_selector))\n",
    "            article_elements = driver.find_elements(*article_link_selector)\n",
    "            article_urls = sorted(list(set([el.get_attribute('href') for el in article_elements])))\n",
    "            print(f\">>> Success! Found {len(article_urls)} unique articles to scrape.\")\n",
    "        except TimeoutException:\n",
    "            print(\"\\n--- FAILURE ---\")\n",
    "            print(\"Could not find any links containing '煤炭库存'. The page structure may have changed.\")\n",
    "            return None \n",
    "\n",
    "        # --- Step 2: Loop through each URL and scrape data with the robust method ---\n",
    "        for i, url in enumerate(article_urls):\n",
    "            print(f\"\\n--- Scraping article {i+1} of {len(article_urls)} ---\")\n",
    "            print(f\"URL: {url}\")\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                \n",
    "                # *** THIS IS THE KEY FIX ***\n",
    "                # Instead of waiting for a specific element, just wait for the page to be ready.\n",
    "                # A small, fixed wait is a simple and effective way to handle pages with different structures.\n",
    "                time.sleep(3) # Wait 3 seconds for the page to render fully.\n",
    "                \n",
    "                # Grab all text from the body of the page. This is guaranteed to exist.\n",
    "                page_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "\n",
    "                # The flexible regex to find \"[Port Name]煤炭库存为[Number]\"\n",
    "                pattern = re.compile(r\"([\\u4e00-\\u9fa5]+港)煤炭库存为(\\d+\\.?\\d*)\")\n",
    "                match = pattern.search(page_text)\n",
    "                \n",
    "                if match:\n",
    "                    port_name = match.group(1).strip()\n",
    "                    inventory = float(match.group(2))\n",
    "                    \n",
    "                    all_results.append({\"Port\": port_name, \"Inventory (10k tons)\": inventory})\n",
    "                    print(f\"  >>> SUCCESS: Found '{port_name}' with inventory {inventory}\")\n",
    "                else:\n",
    "                    print(f\"  > INFO: No data matching the pattern was found in this article's text.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # This provides a much more helpful error message.\n",
    "                print(f\"  > ERROR: An error occurred processing this article: {type(e).__name__} - {e}\")\n",
    "        \n",
    "        if not all_results:\n",
    "            print(\"\\nScraping finished, but no usable data could be extracted.\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(all_results)\n",
    "        df['Scrape Timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df.drop_duplicates(subset=['Port', 'Inventory (10k tons)'], keep='first', inplace=True)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nA critical error occurred: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"\\nScript finished. Closing browser...\")\n",
    "            driver.quit()\n",
    "\n",
    "# --- Run the scraper ---\n",
    "inventory_data = scrape_all_inventory_data_final()\n",
    "\n",
    "# --- Display the final DataFrame ---\n",
    "if inventory_data is not None and not inventory_data.empty:\n",
    "    print(\"\\n\\n--- SCRAPING COMPLETE ---\")\n",
    "    display(inventory_data) \n",
    "else:\n",
    "    print(\"\\n\\n--- SCRAPING FAILED OR NO DATA FOUND ---\")\n",
    "    print(\"Please review the messages above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
